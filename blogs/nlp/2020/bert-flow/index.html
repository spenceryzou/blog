<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>BERT-Flow | Blog</title>
    <meta name="generator" content="VuePress 1.8.2">
    <link rel="icon" href="/blog/icon48.png">
    <link id="echarts-lib" rel="prefetch" href="https://cdn.bootcss.com/echarts/4.2.1/echarts.min.js">
    <meta name="description" content="Blogs on Text Generation, Machine Translation, and other AI related topics">
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no">
    
    <link rel="preload" href="/blog/assets/css/0.styles.d2b9ce88.css" as="style"><link rel="preload" href="/blog/assets/js/app.142e60dd.js" as="script"><link rel="preload" href="/blog/assets/js/3.7e7436de.js" as="script"><link rel="preload" href="/blog/assets/js/1.0068ac03.js" as="script"><link rel="preload" href="/blog/assets/js/8.05ece7ae.js" as="script"><link rel="prefetch" href="/blog/assets/js/10.d0647220.js"><link rel="prefetch" href="/blog/assets/js/11.3fde48e3.js"><link rel="prefetch" href="/blog/assets/js/12.9cdf2fb9.js"><link rel="prefetch" href="/blog/assets/js/13.760303af.js"><link rel="prefetch" href="/blog/assets/js/14.969df60f.js"><link rel="prefetch" href="/blog/assets/js/4.19a78c93.js"><link rel="prefetch" href="/blog/assets/js/5.ecbf661d.js"><link rel="prefetch" href="/blog/assets/js/6.67f1ee16.js"><link rel="prefetch" href="/blog/assets/js/7.9b12ecde.js"><link rel="prefetch" href="/blog/assets/js/9.b5010f5d.js">
    <link rel="stylesheet" href="/blog/assets/css/0.styles.d2b9ce88.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar" data-v-1156296a><div data-v-1156296a><div id="loader-wrapper" class="loading-wrapper" data-v-d48f4d20 data-v-1156296a data-v-1156296a><div class="loader-main" data-v-d48f4d20><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div><div data-v-d48f4d20></div></div> <!----> <!----></div> <div class="password-shadow password-wrapper-out" style="display:none;" data-v-4e82dffc data-v-1156296a data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>Blog</h3> <p class="description" data-v-4e82dffc data-v-4e82dffc>Blogs on Text Generation, Machine Translation, and other AI related topics</p> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Lilei</span>
            
          <span data-v-4e82dffc>2017 - </span>
          2021
        </a></span></div></div> <div class="hide" data-v-1156296a><header class="navbar" data-v-1156296a><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/blog/" class="home-link router-link-active"><img src="/blog/logo.png" alt="Blog" class="logo"> <span class="site-name">Blog</span></a> <div class="links"><!----> <div class="search-box"><i class="iconfont reco-search"></i> <input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav></div></header> <div class="sidebar-mask" data-v-1156296a></div> <aside class="sidebar" data-v-1156296a><div class="personal-info-wrapper" data-v-828910c6 data-v-1156296a><img src="/blog/avatar.png" alt="author-avatar" class="personal-img" data-v-828910c6> <h3 class="name" data-v-828910c6>
    Lilei
  </h3> <div class="num" data-v-828910c6><div data-v-828910c6><h3 data-v-828910c6>4</h3> <h6 data-v-828910c6>Articles</h6></div> <div data-v-828910c6><h3 data-v-828910c6>9</h3> <h6 data-v-828910c6>Tags</h6></div></div> <ul class="social-links" data-v-828910c6></ul> <hr data-v-828910c6></div> <nav class="nav-links"><div class="nav-item"><a href="/blog/" class="nav-link"><i class="iconfont reco-home"></i>
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-category"></i>
      Category
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/categories/MT/" class="nav-link"><i class="undefined"></i>
  MT
</a></li><li class="dropdown-item"><!----> <a href="/blog/categories/NLP/" class="nav-link"><i class="undefined"></i>
  NLP
</a></li></ul></div></div><div class="nav-item"><a href="/blog/tag/" class="nav-link"><i class="iconfont reco-tag"></i>
  Tag
</a></div><div class="nav-item"><a href="/blog/timeline/" class="nav-link"><i class="iconfont reco-date"></i>
  TimeLine
</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title"><i class="iconfont reco-message"></i>
      Contact
    </span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="https://github.com/lileicc" target="_blank" rel="noopener noreferrer" class="nav-link external"><i class="iconfont reco-github"></i>
  lilei
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div></div> <!----></nav> <!----> </aside> <div class="password-shadow password-wrapper-in" style="display:none;" data-v-4e82dffc data-v-1156296a><h3 class="title" data-v-4e82dffc data-v-4e82dffc>BERT-Flow</h3> <!----> <label id="box" class="inputBox" data-v-4e82dffc data-v-4e82dffc><input type="password" value="" data-v-4e82dffc> <span data-v-4e82dffc>Konck! Knock!</span> <button data-v-4e82dffc>OK</button></label> <div class="footer" data-v-4e82dffc data-v-4e82dffc><span data-v-4e82dffc><i class="iconfont reco-theme" data-v-4e82dffc></i> <a target="blank" href="https://vuepress-theme-reco.recoluan.com" data-v-4e82dffc>vuePress-theme-reco</a></span> <span data-v-4e82dffc><i class="iconfont reco-copyright" data-v-4e82dffc></i> <a data-v-4e82dffc><span data-v-4e82dffc>Lilei</span>
            
          <span data-v-4e82dffc>2017 - </span>
          2021
        </a></span></div></div> <div data-v-1156296a><main class="page"><section><div class="page-title"><h1 class="title">BERT-Flow</h1> <div data-v-1ff7123e><i class="iconfont reco-account" data-v-1ff7123e><span data-v-1ff7123e>李博涵</span></i> <i class="iconfont reco-date" data-v-1ff7123e><span data-v-1ff7123e>11/4/2020</span></i> <!----> <i class="tags iconfont reco-tag" data-v-1ff7123e><span class="tag-item" data-v-1ff7123e>AI</span><span class="tag-item" data-v-1ff7123e>NLP</span><span class="tag-item" data-v-1ff7123e>BERT</span><span class="tag-item" data-v-1ff7123e>EMNLP</span></i></div></div> <div class="theme-reco-content content__default"><p><strong>On the Sentence Embeddings from Pre-trained Language Models</strong></p> <h2 id="background"><a href="#background" class="header-anchor">#</a> Background</h2> <p>Recently, pre-trained language models and its variants like BERT have been widely used as representations of natural language.</p> <p><img src="/blog/assets/img/image1.53bd3a5b.png" alt="image1"></p> <p>Photo credit to https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/</p> <p>Despite their great success on many NLP tasks through fine-tuning, the sentence embeddings from BERT without finetuning are significantly inferior in terms of semantic textual similarity (Reimers and Gurevych, 2019) – for example, they even underperform the GloVe embeddings which are not contextualized and trained with a much simpler model. Such issues hinder applying BERT sentence embeddings directly to many real-world scenarios where collecting labeled data is highlycosting or even intractable.</p> <p><img src="/blog/assets/img/image2.51db4c2c.png" alt="image2"></p> <h2 id="major-questions"><a href="#major-questions" class="header-anchor">#</a> Major Questions</h2> <p>In this paper, we aim to answer two major questions:</p> <ul><li><p>(1) why do the BERT-induced sentence embeddings perform poorly to retrieve semantically similar sentences? Do they carry too little semantic information, or just because the semantic meanings in these embeddings are not exploited properly?</p></li> <li><p>(2) If the BERT embeddings capture enough semantic information that is hard to be directly utilized, how can we make it easier without external supervision?</p></li></ul> <h2 id="our-findings"><a href="#our-findings" class="header-anchor">#</a> Our Findings</h2> <p>We argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity.</p> <h3 id="the-anisotropic-embedding-space-of-bert"><a href="#the-anisotropic-embedding-space-of-bert" class="header-anchor">#</a> The Anisotropic Embedding Space of BERT</h3> <p>Gao et al. (2019) and Wang et al. (2020) have pointed out that, for language modeling, the maximum likelihood training with Equation 1 usually produces an anisotropic word embedding space. “Anisotropic” means word embeddings occupy a narrow cone in the vector space. This phenomenon is also observed in the pretrained Transformers like BERT, GPT-2, etc (Ethayarajh, 2019).</p> <p><img src="/blog/assets/img/image3.d7d0ce5a.png" alt="image3"></p> <div class="custom-block tip"><p class="title"></p><p>The BERT word embedding space. The 2D-scatterplot is achieved via SVD-based dimension reduction. The embeddings are colored according to their associated word frequency.</p></div><h3 id="word-frequency-biases-the-embedding-space"><a href="#word-frequency-biases-the-embedding-space" class="header-anchor">#</a> Word Frequency Biases the Embedding Space</h3> <p>However, as discussed by Gao et al. (2019), anisotropy is highly relevant to the imbalance of word frequency. We observe that high-frequency words are all close to the origin, while low-frequency words are far away from the origin.</p> <p>This phenomenon can be explained through the softmax formulation of (masked) language models. Note that there is a word-frequency term in the decomposition of the dot product between context and word embeddings. Nevertheless, the PMI term is still highly associated with semantic similarity.</p> <div style="text-align:center;"><img src="/blog/assets/img/image4.ac5ebe3c.png" width="400"> <img src="/blog/assets/img/image5.1102d40e.png" width="400"></div> <div class="custom-block warning"><p class="title">Remark</p><p>We expect the embedding induced similarity to be consistent to semantic similarity.  If embeddings are distributed in different regions according to frequency statistics, the induced similarity is not useful any more.</p></div><p><img src="/blog/assets/img/image6.2543e8da.png" alt="image6"></p> <h3 id="low-frequency-words-disperse-sparsely"><a href="#low-frequency-words-disperse-sparsely" class="header-anchor">#</a> Low-Frequency Words Disperse Sparsely</h3> <p>We also observe that, in the learned anisotropic embedding space, high-frequency words concentrates densely to their k-nearest neighbors and low-frequency words disperse sparsely.</p> <p><img src="/blog/assets/img/image7.3f0e827a.png" alt="image7"></p> <div class="custom-block warning"><p class="title">Remark</p><p>Due to the sparsity, many “holes” could be formed around the low-frequency words in the embedding space, where the semantic meaning can be poorly defined.</p></div><h2 id="proposed-method-bert-flow"><a href="#proposed-method-bert-flow" class="header-anchor">#</a> Proposed Method: BERT-flow</h2> <p>To address these issues, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective.</p> <p>A standard Gaussian latent space may have favorable properties which can help with our problem.</p> <ul><li>First, standard Gaussian satisfies isotropy.  By fitting a mapping to an isotropic distribution, the singular spectrum of the embedding space can be flattened. In this way, the word frequency-related singular directions, which are the dominating ones, can be suppressed.</li> <li>Second, the probabilistic density of Gaussian is well defined over the entire real space. This means there are no “hole” areas, which are poorly defined in terms of probability. The helpfulness of Gaussian prior for mitigating the “hole” problem has been widely observed in existing literature of deep latent variable models (e.g., variational auto-encoders).</li></ul> <p><img src="/blog/assets/img/image8.db30554f.png" alt="image8"></p> <h2 id="experiments"><a href="#experiments" class="header-anchor">#</a> Experiments</h2> <p>Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks.</p> <h3 id="results-w-o-nli-supervision"><a href="#results-w-o-nli-supervision" class="header-anchor">#</a> Results w/o NLI Supervision</h3> <p>We perform extensive experiments on 7 standard semantic textual similarity benchmarks without using any downstream supervision. Our empirical results demonstrate that the flow transformation is able to consistently improve BERT by up to 12.70 points with an average of 8.16 points in terms of Spearman correlation between cosine embedding similarity and human annotated similarity.</p> <p><img src="/blog/assets/img/image9.3426ecab.png" alt="image9"></p> <h3 id="results-w-nli-supervision"><a href="#results-w-nli-supervision" class="header-anchor">#</a> Results w/ NLI Supervision</h3> <p>When combined with external supervision from NLI tasks, our method outperforms the <strong>sentence-BERT</strong> embeddings (Reimers and Gurevych, 2019), leading to new state-of-theart performance.</p> <p><img src="/blog/assets/img/image10.3e196813.png" alt="image10"></p> <h2 id="conclusion"><a href="#conclusion" class="header-anchor">#</a> Conclusion</h2> <p>We investigate the deficiency of the BERT sentence embeddings on semantic textual similarity. We propose a flow-based calibration which can effectively improve the performance. BERT-flow obtains significant performance gains over the SoTA sentence embeddings on a variety of semantic textual similarity tasks.</p> <h2 id="reference"><a href="#reference" class="header-anchor">#</a> Reference</h2> <ul><li>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using siamese BERT networks. In Proceedings of EMNLP-IJCNLP.</li> <li>Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and TieYan Liu. 2019. Representation degeneration problem in training natural language generation models. In Proceedings of ICLR.</li> <li>Lingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu, Guangtao Wang, and Quanquan Gu. 2020. Improving neural language generation with spectrum control. In Proceedings of ICLR.</li> <li>Kawin Ethayarajh. 2019. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. In Proceedings of EMNLP-IJCNLP.</li></ul></div></section> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated: </span> <span class="time">6/21/2021, 2:29:32 PM</span></div></footer> <!----> <div class="comments-wrapper"><!----></div> <ul class="side-bar sub-sidebar-wrapper" style="width:12rem;" data-v-70334359><li class="level-2" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#background" class="sidebar-link reco-side-background" data-v-70334359>Background</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#major-questions" class="sidebar-link reco-side-major-questions" data-v-70334359>Major Questions</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#our-findings" class="sidebar-link reco-side-our-findings" data-v-70334359>Our Findings</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#the-anisotropic-embedding-space-of-bert" class="sidebar-link reco-side-the-anisotropic-embedding-space-of-bert" data-v-70334359>The Anisotropic Embedding Space of BERT</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#word-frequency-biases-the-embedding-space" class="sidebar-link reco-side-word-frequency-biases-the-embedding-space" data-v-70334359>Word Frequency Biases the Embedding Space</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#low-frequency-words-disperse-sparsely" class="sidebar-link reco-side-low-frequency-words-disperse-sparsely" data-v-70334359>Low-Frequency Words Disperse Sparsely</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#proposed-method-bert-flow" class="sidebar-link reco-side-proposed-method-bert-flow" data-v-70334359>Proposed Method: BERT-flow</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#experiments" class="sidebar-link reco-side-experiments" data-v-70334359>Experiments</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#results-w-o-nli-supervision" class="sidebar-link reco-side-results-w-o-nli-supervision" data-v-70334359>Results w/o NLI Supervision</a></li><li class="level-3" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#results-w-nli-supervision" class="sidebar-link reco-side-results-w-nli-supervision" data-v-70334359>Results w/ NLI Supervision</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#conclusion" class="sidebar-link reco-side-conclusion" data-v-70334359>Conclusion</a></li><li class="level-2" data-v-70334359><a href="/blog/blogs/nlp/2020/bert-flow/#reference" class="sidebar-link reco-side-reference" data-v-70334359>Reference</a></li></ul></main> <!----></div></div></div></div><div class="global-ui"><div class="back-to-ceiling" style="right:1rem;bottom:6rem;width:2.5rem;height:2.5rem;border-radius:.25rem;line-height:2.5rem;display:none;" data-v-c6073ba8 data-v-c6073ba8><svg t="1574745035067" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5404" class="icon" data-v-c6073ba8><path d="M526.60727968 10.90185116a27.675 27.675 0 0 0-29.21455937 0c-131.36607665 82.28402758-218.69155461 228.01873535-218.69155402 394.07834331a462.20625001 462.20625001 0 0 0 5.36959153 69.94390903c1.00431239 6.55289093-0.34802892 13.13561351-3.76865779 18.80351572-32.63518765 54.11355614-51.75690182 118.55860487-51.7569018 187.94566865a371.06718723 371.06718723 0 0 0 11.50484808 91.98906777c6.53300375 25.50556257 41.68394495 28.14064038 52.69160883 4.22606766 17.37162448-37.73630017 42.14135425-72.50938081 72.80769204-103.21549295 2.18761121 3.04276886 4.15646224 6.24463696 6.40373557 9.22774369a1871.4375 1871.4375 0 0 0 140.04691725 5.34970492 1866.36093723 1866.36093723 0 0 0 140.04691723-5.34970492c2.24727335-2.98310674 4.21612437-6.18497483 6.3937923-9.2178004 30.66633723 30.70611158 55.4360664 65.4791928 72.80769147 103.21549355 11.00766384 23.91457269 46.15860503 21.27949489 52.69160879-4.22606768a371.15156223 371.15156223 0 0 0 11.514792-91.99901164c0-69.36717486-19.13165746-133.82216804-51.75690182-187.92578088-3.42062944-5.66790279-4.76302748-12.26056868-3.76865837-18.80351632a462.20625001 462.20625001 0 0 0 5.36959269-69.943909c-0.00994388-166.08943902-87.32547796-311.81420293-218.6915546-394.09823051zM605.93803103 357.87693858a93.93749974 93.93749974 0 1 1-187.89594924 6.1e-7 93.93749974 93.93749974 0 0 1 187.89594924-6.1e-7z" p-id="5405" data-v-c6073ba8></path><path d="M429.50777625 765.63860547C429.50777625 803.39355007 466.44236686 1000.39046097 512.00932183 1000.39046097c45.56695499 0 82.4922232-197.00623328 82.5015456-234.7518555 0-37.75494459-36.9345906-68.35043303-82.4922232-68.34111062-45.57627738-0.00932239-82.52019037 30.59548842-82.51086798 68.34111062z" p-id="5406" data-v-c6073ba8></path></svg></div></div></div>
    <script src="/blog/assets/js/app.142e60dd.js" defer></script><script src="/blog/assets/js/3.7e7436de.js" defer></script><script src="/blog/assets/js/1.0068ac03.js" defer></script><script src="/blog/assets/js/8.05ece7ae.js" defer></script>
  </body>
</html>
