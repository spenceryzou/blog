---
title: SentencePiece Tokenizing and Detokenizing Review
author: Spencer Zou
date: 2021-12-01
tags:
 - NLP
 - SentencePiece
 - Tokenizer
 - Detokenizer
 - Subword
categories:
 - NLP
---

**SentencePiece Tokenizing and Detokenizing Review**

<!-- more -->

## Background

SentencePiece is an open-source tokenizer and detokenizer often used for Neural Machine Translation systems where the vocabulary size is fixed. SentencePiece takes in any raw sentence input and generates subword units for the sentence. Unlike other subword tokenizer tools, SentencePiece does not require sentences to be pretokenized by word allowing it to be language independent, working on languages like Japanese and Chinese that do not have whitespaces to segment words.

![sentencepiece.png](./sentencepiece.png)

## About Subword Tokenization

Before we tackle a Natural Language Processing problem such as translation, we require pre-processing on the input data to make the information more useful for a computer. One step of pre-processing is tokenization, which is the splitting of a sentence into smaller units. Different tokenization tools will split sentences in different ways. Word tokenization merely splits a sentence by each word, for instance seperating by whitespace for most European-based languages. The issue with word tokenization is that often these vocabulary sizes get too large as a token needs to be created for every word, even if words have similar meanings.

**"This is how we do tokenization." becomes *["This","is","how","we","do","tokenization."]**

For most state-of-the-art models, subword tokenization is used. This splits each word into smaller subwords. This allows large words with similar prefixes, suffixes, or components to be split into units that convey a specific meaning. For instance, **tokenization** can be split into **token** and **ization**, which shares the **ization** token with a word like **colonization**. 

## Challenges

One challenge that subword tokenization faces is that most tools require the input to be pretokenized into words. This means that before subword tokenization can be done, a word tokenizer needs to be used on the sentence, which depends on the language of the sentence. For instance a word tokenizer for a European language that splits on whitespace cannot be used on Japanese, which does not have whitespaces. Instead specific rules need to be used which are difficult to maintain and resource expensive to process.

- Raw text: こんにちは世界。(Hello world.)
- Tokenized: [こんにちは] [世界] [。]

Another challenge that subword tokenization faces is that with word pre-tokenization, there is some information loss when performing detokenization.

- Raw text: Hello world.
- Tokenized: [Hello] [world] [.]

Here this tokenization is not lossless as there is no information in the tokens that indicates that no space should be put between the word world and the period punctuation. Instead language-dependent detokenization rules need to be implemented. With these two challenges, it is difficult to have a subword tokenization that fits into an end-to-end language-independent system.

## SentencePiece to the Rescue

To tackle the challenges, we can use **SentencePiece**. SentencePiece is unique from common subword tokenizers like subword-nmt in that it takes a fixed vocabulary size as a variable. In contrast, subword-nmt takes in a number of merge operations as a variable to be used for its underlying Byte-Pair Encoding algorithm. SentencePiece is composed of four parts: Normaliser, Trainer, Encoder, and Decoder.

![image1](./image1.png)

**Normaliser**. Text is normalized by standardizing it to follow a suitable format. For SentencePiece, the default is NFKC Unicode. This means things that have the same meaning regardless of accents on letters are made to be the same character.

**Trainer**. A vocabulary is generated by creating subword components through either BPE or unigram language model. The input is a raw stream of Unicode characters rather than tokenized words. This means things like whitespaces are treated as a character themself by representing it as "\_" This solves the problem of information loss when performing detokenization. In addition, this also means languages like Japanese without whitespaces can be tokenized in the same way as each Japanese character is treated individually.

- Raw  text: Hello world.
- Tokenized: [Hello] [▁Wor] [ld] [.]

The above example can have lossless detokenization, being put back together completely without any additional rules. SentencePiece is able to process this at a character level with comparable speed since it uses a priority-queue based BPE algorithm instead of the naive scan of all pairs normally used with BPE. This reduces the algorithmic runtime from [equation](https://latex.codecogs.com/gif.latex?O%28nlogn%29) to [equation](https://latex.codecogs.com/gif.latex?O%28n%5E%7B2%7D%29).

**Encoder and Decoder**. Encoding and decoding just refer to applying the subword vocabulary on the text, and converting it back to the full text respectively.

Finally, SentencePiece has self-contained models with no external dependencies. This means that SentencePiece with the same model file run with the same parameters on the same dataset will have the same result. 

## Experiments

### Dataset

We evaluate our model on different 

### Main Results

We compare the performance among the following models:

- **CNN**, **LSTM**, **BiLSTM**, **Context-Aware**, **BERT-RE**, **RoBERTa-RE**, **CorefBERT-RE**, **CorefRoBERTa-RE**: Using different encoding mechanisms to simply encode the whole document and extract relations.
- **HIN-Glove**, **HIN-BERT**: Extracting relations through a hierarchical interaction network with either Glove embedding or BERT.
- **GAT**, **GCNN**, **EOG**, **AGGCN**, **LSR-Glove**, **LSR-BERT**: Previous graph-based methods, while our graph construction is totally different from theirs and they conduct logical reasoning only based on GCN.
- **GAIN-Glove**, **GAIN-BERT**: Our proposed model with either Glove embedding or BERT.

The evaluation metrics we use are F1/AUC and Ign-F1/Ign-AUC. The latter means we do not consider the triples (i.e., head-relation-tail) that are already contained in the training set.

![image5](./image5.png)

The key observations are:
- Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9∼12.82 F1 score on the test set. 
- Among the models using BERT or BERT variants, GAIN-BERT base yields a great improvement of F1/Ign F1 on dev and test set by 2.22/6.71 and 2.19/2.03, respectively, in comparison with the strong baseline LSR-BERT base. GAIN-BERT large also improves 2.85/2.63 F1/Ign F1 on test set compared with
previous state-of-the-art method, CorefRoBERTaRElarge.
- GAIN can better utilize powerful BERT representation. LSR-BERT base improves F1 by 3.83 and 4.87 on dev and test set with GloVe embedding replaced with BERTbase while our GAIN-BERT base yields an improvement by 5.93 and 6.16.

### Ablation Study

We conduct ablation study by removing the mention-level graph, entity-level graph inference module, and the document node in the mention-level graph. The F1 scores on test set significantly decrease by 2.02\~2.34/1.61\~1.90 for GAIN-Glove/GAIN-BERT.

![image6](./image6.png)

### Further Analysis

#### Cross-sentence Relation Extraction

We evaluate GAIN on relations within a single sentence (Intra-F1) and those involving multiple sentences (Inter-F1), respectively. GAIN outperforms other baselines not only in Intra-F1 but also Inter-F1. The removal of Mention-level Graph (hMG) leads to a more considerable decrease in Inter-F1 than Intra-F1, which indicates
our hMG do help interactions among mentions, especially those distributed in different sentences with long-distance dependency.

<div align=center><img src="./image7.png"></div>

#### Logical Reasoning for Relation Extraction

We evaluate GAIN on relations requiring logical reasoning (Infer-F1), and the experimental results show GAIN can better handle relational inference. For example, GAIN-BERT base improves 5.11 Infer-F1 compared with RoBERTa-RE base. The inference module also plays an important role in capturing potential inference chains between entities, without which GAIN-BERT base would drop by 1.78 Infer-F1.

<div align=center><img src="./image8.png"></div>

### Case Study

The figure above shows the case study of our proposed model GAIN, in comparison with other baselines. As is shown, BiLSTM can only identify two relations within the first sentence. Both BERT-RE base and GAIN-BERT base can successfully predict **Without Me** is part of **The Eminem Show**. But only GAIN-BERT base is able to deduce the performer and publication date of **Without Me** are the same as those of **The Eminem Show**, namely **Eminem** and **May 26, 2002**, where it requires logical inference across sentences.

![image9](./image9.png)

## Conclusion

Extracting inter-sentence relations and conducting relational reasoning are challenging in document-level relation extraction. In this paper, we introduce Graph Aggregationand-Inference Network (GAIN) to better cope with document-level relation extraction, which features double graphs in different granularity. GAIN
utilizes a heterogeneous Mention-level Graph to model the interaction among different mentions across the document and capture document-aware features. It also uses an Entity-level Graph with a proposed path reasoning mechanism to infer relations more explicitly. Experimental results on the large-scale human annotated dataset, DocRED, show GAIN outperforms previous methods, especially in inter-sentence and inferential relations scenarios. The ablation study also confirms the effectiveness of different modules in our model.

## Reference 

- Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou, Maosong Sun. 2019. DocRED: A Large-Scale Document-Level Relation Extraction Dataset. In Proceedings of ACL. 
